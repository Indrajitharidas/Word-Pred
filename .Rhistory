?power.t.test
power.t.test(delta=0.01, sd=0.04, power=0.9)
power.t.test(delta=0.01, sd=0.04, power=0.9, alternative="one.sided")
power.t.test(delta=0.01, sd=0.04, power=0.9, alternative="one.sided", type = "one.sample")
power.t.test(delta=0.01, sd=0.04, n=100, alternative="one.sided", type = "one.sample")
upgrade()
install.packages(knitr)
install.packages("knitr")
source('D:/Github/SwiftKey_Prediction/Exploration.R')
zData <- tm_map(zData, stripWhitespace)
zdata <- tm_map(zdata, stripWhitespace)
zdata[3]$content[1]
zdata[3]$content[1][1:30]
zdata <- tm_map(zdata, tmTolower)
?tmToLower
ls("package:tm")
profanity <- read.csv("Profanity.csv")
getwd()
profanity <- read.csv("ProfanityWords.csv")
View(profanity)
profanity <- read.csv("ProfanityWords.csv", header=FALSE)
View(profanity)
profanity <- profanity$V1
?removeWords
?stripWhitespace
zdata <- tm_map(zdata, content_transformer(tolower))
?removeWords
source('D:/Github/SwiftKey_Prediction/Exploration.R')
source('D:/Github/SwiftKey_Prediction/Exploration.R')
?readDoc
?readDOC
zData2 <- Corpus(DataframeSource(Data.Frame(data_blogs)), readerControl = list(language="en_US"))
zData2 <- Corpus(DataframeSource(data.frame(data_blogs)), readerControl = list(language="en_US"))
zData2 <- Corpus(VectorSource(data_blogs), readerControl = list(language="en_US"))
remove(zData2)
length(data_blogs)
data_blogs[899288]
length(data_blogs) + length(data_news) + length(data_twitter)
20000/3336695
?rbinom
rbinom(10,1,0.5)
rbinom(10,2,0.5)
rbinom(10,2,0.01)
rbinom(10,2,0.01)
rbinom(10,2,0.01)
rbinom(10,2,0.01)
sum(rbinom(100,2,0.01))
factor(rbinom(10,2,0.5), levels = c(TRUE, FALSE))
?factor
factor(rbinom(10,2,0.5), c(1,0), levels = c(TRUE, FALSE))
factor(rbinom(10,2,0.5), levels=c(1,0), labels = c(TRUE, FALSE))
factor(rbinom(10,2,0.5), levels=c(1,0), labels = c(TRUE, FALSE))
factor(rbinom(10,2,0.5), levels=c(1,0), labels = c(TRUE, FALSE))
factor(rbinom(10,1,0.5), levels=c(1,0), labels = c(TRUE, FALSE))
data <- c(data_blogs, data_news)
?gsub
data_blogs[1:10][factor(rbinom(10,1,0.5), levels=c(1,0), labels = c(TRUE, FALSE))]
data_blogs[1:10]
data_blogs[factor(rbinom(10,1,0.5), levels=c(1,0), labels = c(TRUE, FALSE))]
data_blogs[1:10][rbinom(10,1,0.5)]
data_blogs[1:10][[rbinom(10,1,0.5)]]
data_twitter[1:10][rbinom(10,1,0.5)]
data_blogs[1:10]
data_twitter[1:10]
data_twitter[1:10][c(1,4,10)]
data_twitter[1:10][c(1,0,1)]
data_twitter[1:10][c(TRUE,FALSE)]
data_twitter[1:10][c(factor(rbinom(10,1,0.5), levels=c(1,0), labels = c(TRUE, FALSE)))]
data_twitter[1:10][c(TRUE,FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE)]
c(factor(rbinom(10,1,0.5), levels=c(1,0), labels = c(TRUE, FALSE)))
factor(rbinom(10,1,0.5), levels=c(1,0), labels = c(TRUE, FALSE))
factor(c(rbinom(10,1,0.5)), levels=c(1,0), labels = c(TRUE, FALSE))
x <- factor(c(rbinom(10,1,0.5)), levels=c(1,0), labels = c(TRUE, FALSE))
data_twitter[1:10][x]
?rbinom
?replace
?sub
gsub(TRUE,FALSE, rbinom(10,1,0.5))
x <- rbinom(10,1,0.5)
replace(x, x == 0, FALSE)
x
x <- rbinom(10,5,0.5)
x
x == 1
y <- x == 1
str(y)
as.logical(rbinom(10,5,0.5))
as.logical(rbinom(10,1,0.5))
data_twitter[1:10][as.logical(rbinom(10,1,0.5))]
data_twitter[1:10]
source('D:/Github/SwiftKey_Prediction/Exploration.R')
source('D:/Github/SwiftKey_Prediction/Exploration.R')
source('D:/Github/SwiftKey_Prediction/Exploration.R')
x <- c("asdf", "3ff", "1!o", "awe yea")
?regexpr
regexpr("[^a-zA-Z ]", x)
mapply(x, regexpr("[^a-zA-Z ]"))
mapply(x, regexpr(), "[^a-zA-Z ]")
gsub("[^a-zA-Z ]", "", x)
selectedData[1:10]
selectedData <- gsub("[^a-zA-Z ]", "", selectedData)
selectedData[1:10]
source('D:/Github/SwiftKey_Prediction/Exploration.R')
selectedData[1:10]
?stripWhitespace
zdata[1]
zdata[1]$content
x <- ("bitch", "titties", "help", "emily")
x <- c("bitch", "titties", "help", "emily")
gsub(profanity," ", x)
profanity
?paste
paste(profanity, collapse='|')
gsub(paste(profanity, collapse='|'), " ", x)
source('D:/Github/SwiftKey_Prediction/Exploration.R')
zToken <- DocumentTermMatrix(zdata)
inspect(zToken[5:10, 630:643])
findFreqTerms(zToken, 5)
?removeSparseTerms
inspect(removeSparseTerms(zToken, 0.4))
inspect(removeSparseTerms(zToken, 0.1))
inspect(removeSparseTerms(zToken, 0.01))
findFreqTerms(zToken, 100)
findFreqTerms(zToken, 1000)
inspect(tm)
li(tm)
ls("package:tm")
?scan_tokenizer
findFreqTerms
?findFreqTerms
